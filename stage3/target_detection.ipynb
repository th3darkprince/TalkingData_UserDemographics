{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_events = pd.read_pickle(\"dataset/train_events.pkl\")\n",
    "df_val_events = pd.read_pickle(\"dataset/test_events.pkl\")\n",
    "\n",
    "df_train_noevents = pd.read_pickle(\"dataset/train_noevents.pkl\")\n",
    "df_val_noevents = pd.read_pickle(\"dataset/test_noevents.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_train, df_events_test = train_test_split(df_train_events, test_size = 0.2)\n",
    "df_noevents_train, df_noevents_test = train_test_split(df_train_noevents, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(classifier, hp, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"Function to perform Hyperparameter tuning of the models. Takes the model name, hyperparameter name and list \n",
    "    along with the train, cross validation and test datasets as input\"\"\"\n",
    "    \n",
    "\n",
    "    if classifier == \"lrb\":\n",
    "        clf = SGDClassifier()\n",
    "    elif classifier == \"svm\":\n",
    "        clf = SGDClassifier()\n",
    "    elif classifier == \"rf\":\n",
    "        clf = RandomForestClassifier()\n",
    "    \n",
    "    \n",
    "    random_clf=RandomizedSearchCV(clf, param_distributions=hp, verbose=10, cv=3, n_jobs=8)\n",
    "    random_clf.fit(x_train, y_train)\n",
    "    cv_log_error_array = []\n",
    "    \n",
    "    clf_best = random_clf.best_estimator_\n",
    "    \n",
    "    sig_clf = CalibratedClassifierCV(clf_best, method=\"sigmoid\")\n",
    "    sig_clf.fit(x_train, y_train)\n",
    "    \n",
    "    predict_y = sig_clf.predict_proba(x_test)\n",
    "    score = log_loss(y_test, predict_y)\n",
    "    print(\"The test log loss is:\",score)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_vec = TfidfVectorizer()\n",
    "brand_train = brand_vec.fit_transform(df_noevents_train['brand'])\n",
    "brand_test = brand_vec.transform(df_noevents_test['brand'])\n",
    "\n",
    "model_vec = TfidfVectorizer()\n",
    "model_train = model_vec.fit_transform(df_noevents_train['model'])\n",
    "model_test = model_vec.transform(df_noevents_test['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing required inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "age_train = scaler.fit_transform(df_noevents_train['pred_age'].values.reshape(-1,1))\n",
    "age_test = scaler.transform(df_noevents_test['pred_age'].values.reshape(-1,1))\n",
    "\n",
    "screen_train = scaler.fit_transform(df_noevents_train['screen_size'].values.reshape(-1,1))\n",
    "screen_test = scaler.transform(df_noevents_test['screen_size'].values.reshape(-1,1))\n",
    "\n",
    "ram_train = scaler.fit_transform(df_noevents_train['ram_gb'].values.reshape(-1,1))\n",
    "ram_test = scaler.transform(df_noevents_test['ram_gb'].values.reshape(-1,1))\n",
    "\n",
    "camera_train = scaler.fit_transform(df_noevents_train['camera'].values.reshape(-1,1))\n",
    "camera_test = scaler.transform(df_noevents_test['camera'].values.reshape(-1,1))\n",
    "\n",
    "release_train = scaler.fit_transform(df_noevents_train['release_bin'].values.reshape(-1,1))\n",
    "release_test = scaler.transform(df_noevents_test['release_bin'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = hstack((brand_train, model_train, df_noevents_train['female_pred'].values.reshape(-1,1), \n",
    "                  df_noevents_train['male_pred'].values.reshape(-1,1), age_train, screen_train, ram_train, \n",
    "                  camera_train, release_train))\n",
    "x_test = hstack((brand_test, model_test, df_noevents_test['female_pred'].values.reshape(-1,1), \n",
    "                 df_noevents_test['male_pred'].values.reshape(-1,1), age_test, screen_test, ram_test,\n",
    "                 camera_test, release_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoder = LabelEncoder()\n",
    "\n",
    "y_train = y_encoder.fit_transform(df_noevents_train['group'])\n",
    "y_test = y_encoder.transform(df_noevents_test['group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "noevents_model_rand = \"No Events - Random Model\"\n",
    "\n",
    "predicted_y = np.zeros((len(y_test),12))\n",
    "for i in range(len(y_test)):\n",
    "    rand_probs = np.random.rand(1,12)\n",
    "    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
    "\n",
    "noevents_rand = log_loss(y_test, predicted_y, labels = list(range(12)), eps=1e-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "noevents_model_lrb = \"No Events - Logistic Regression\"\n",
    "\n",
    "hp = {'alpha':[10 ** x for x in range(-6, 3)]}\n",
    "\n",
    "noevents_lrb = hyperparameter_tuning(\"lrb\", hp, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\sklearn\\model_selection\\_search.py:281: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=8)]: Done  15 out of  27 | elapsed:    7.8s remaining:    6.2s\n",
      "[Parallel(n_jobs=8)]: Done  18 out of  27 | elapsed:    8.7s remaining:    4.3s\n",
      "[Parallel(n_jobs=8)]: Done  21 out of  27 | elapsed:    9.2s remaining:    2.6s\n",
      "[Parallel(n_jobs=8)]: Done  24 out of  27 | elapsed:    9.3s remaining:    1.1s\n",
      "[Parallel(n_jobs=8)]: Done  27 out of  27 | elapsed:   11.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  27 out of  27 | elapsed:   11.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test log loss is: 2.4178723686862655\n"
     ]
    }
   ],
   "source": [
    "noevents_model_svm = \"No Events - Support Vector Machines\"\n",
    "\n",
    "c = {'c':[10 ** x for x in range(-5, 3)]}\n",
    "\n",
    "noevents_svm = hyperparameter_tuning(\"svm\", hp, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "noevents_model_rf = \"No Events - Random Forest\"\n",
    "\n",
    "hp = {'n_estimators':[100,200,500,1000,2000],'max_depth':[5, 10]}\n",
    "\n",
    "noevents_rf = hyperparameter_tuning(\"rf\", hp, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:   39.0s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=8)]: Done  19 out of  30 | elapsed:  6.3min remaining:  3.6min\n",
      "[Parallel(n_jobs=8)]: Done  23 out of  30 | elapsed:  6.8min remaining:  2.1min\n",
      "[Parallel(n_jobs=8)]: Done  27 out of  30 | elapsed:  7.2min remaining:   48.2s\n",
      "[Parallel(n_jobs=8)]: Done  30 out of  30 | elapsed: 10.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n...\n",
       "                                           verbosity=None),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=8,\n",
       "                   param_distributions={'colsample_bytree': [0.1, 0.3, 0.5, 1],\n",
       "                                        'learning_rate': [0.01, 0.03, 0.05, 0.1,\n",
       "                                                          0.15, 0.2],\n",
       "                                        'max_depth': [3, 5, 10],\n",
       "                                        'n_estimators': [100, 200, 500, 1000,\n",
       "                                                         2000],\n",
       "                                        'subsample': [0.1, 0.3, 0.5, 1]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='neg_log_loss',\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noevents_model_xgb = \"No Events - XGBoost\"\n",
    "\n",
    "x_clf=XGBClassifier()\n",
    "prams={\n",
    "'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n",
    "'n_estimators':[100,200,500,1000,2000],\n",
    "'max_depth':[3,5,10],\n",
    "'colsample_bytree':[0.1,0.3,0.5,1],\n",
    "'subsample':[0.1,0.3,0.5,1]\n",
    "}\n",
    "random_clf=RandomizedSearchCV(x_clf, param_distributions=prams, verbose=10, cv=3, n_jobs=8, scoring = 'neg_log_loss')\n",
    "random_clf.fit(x_train, y_train)\n",
    "\n",
    "x_clf_best = random_clf.best_estimator_\n",
    "predict_y = x_clf_best.predict_proba(x_test)\n",
    "noevents_xgb = log_loss(y_test, predict_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=12\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(nodes):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(2048, input_dim=nodes, init='normal', activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, input_dim=nodes, init='normal', activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12, init='normal', activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, input_dim=1333, activation=\"tanh\", kernel_initializer=\"normal\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, input_dim=1333, activation=\"tanh\", kernel_initializer=\"normal\")`\n",
      "  import sys\n",
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 41084 samples, validate on 10271 samples\n",
      "Epoch 1/15\n",
      "41084/41084 [==============================] - 4s 97us/step - loss: 2.4410 - accuracy: 0.1296 - val_loss: 2.4352 - val_accuracy: 0.1308\n",
      "Epoch 2/15\n",
      "41084/41084 [==============================] - 3s 72us/step - loss: 2.4346 - accuracy: 0.1330 - val_loss: 2.4337 - val_accuracy: 0.1308\n",
      "Epoch 3/15\n",
      "41084/41084 [==============================] - 3s 72us/step - loss: 2.4338 - accuracy: 0.1336 - val_loss: 2.4353 - val_accuracy: 0.1308\n",
      "Epoch 4/15\n",
      "41084/41084 [==============================] - 3s 72us/step - loss: 2.4336 - accuracy: 0.1336 - val_loss: 2.4382 - val_accuracy: 0.1308\n",
      "Epoch 5/15\n",
      "41084/41084 [==============================] - 3s 72us/step - loss: 2.4323 - accuracy: 0.1336 - val_loss: 2.4356 - val_accuracy: 0.1308\n",
      "Epoch 6/15\n",
      "41084/41084 [==============================] - 3s 73us/step - loss: 2.4310 - accuracy: 0.1335 - val_loss: 2.4308 - val_accuracy: 0.1308\n",
      "Epoch 7/15\n",
      "41084/41084 [==============================] - 3s 73us/step - loss: 2.4313 - accuracy: 0.1333 - val_loss: 2.4318 - val_accuracy: 0.1308\n",
      "Epoch 8/15\n",
      "41084/41084 [==============================] - 3s 72us/step - loss: 2.4313 - accuracy: 0.1330 - val_loss: 2.4302 - val_accuracy: 0.1308\n",
      "Epoch 9/15\n",
      "41084/41084 [==============================] - 3s 72us/step - loss: 2.4304 - accuracy: 0.1341 - val_loss: 2.4342 - val_accuracy: 0.1308\n",
      "Epoch 10/15\n",
      "41084/41084 [==============================] - 3s 72us/step - loss: 2.4299 - accuracy: 0.1338 - val_loss: 2.4302 - val_accuracy: 0.1317\n",
      "Epoch 11/15\n",
      "41084/41084 [==============================] - 3s 72us/step - loss: 2.4297 - accuracy: 0.1333 - val_loss: 2.4291 - val_accuracy: 0.1308\n",
      "Epoch 12/15\n",
      "41084/41084 [==============================] - 3s 72us/step - loss: 2.4303 - accuracy: 0.1332 - val_loss: 2.4283 - val_accuracy: 0.1308\n",
      "Epoch 13/15\n",
      "41084/41084 [==============================] - 3s 73us/step - loss: 2.4294 - accuracy: 0.1342 - val_loss: 2.4298 - val_accuracy: 0.1308\n",
      "Epoch 14/15\n",
      "41084/41084 [==============================] - 3s 72us/step - loss: 2.4300 - accuracy: 0.1336 - val_loss: 2.4292 - val_accuracy: 0.1308\n",
      "Epoch 15/15\n",
      "41084/41084 [==============================] - 3s 72us/step - loss: 2.4292 - accuracy: 0.1344 - val_loss: 2.4281 - val_accuracy: 0.1308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x126a6516780>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model=baseline_model(x_train.shape[1])\n",
    "model.fit(x_train, y_train, epochs=15, batch_size = 64, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test log loss is: 2.428108946999212\n"
     ]
    }
   ],
   "source": [
    "predict_y = model.predict_proba(x_test)\n",
    "print(\"The test log loss is:\",log_loss(y_test, predict_y))\n",
    "noevents_nn = log_loss(y_test, predict_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for models with No Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+--------------------+\n",
      "|                Model                |      LogLoss       |\n",
      "+-------------------------------------+--------------------+\n",
      "|       No Events - Random Model      | 2.770839388865056  |\n",
      "|   No Events - Logistic Regression   | 2.417876851355263  |\n",
      "| No Events - Support Vector Machines | 2.4178723686862655 |\n",
      "|      No Events - Random Forest      | 2.3940721530512206 |\n",
      "|         No Events - XGBoost         | 2.396952661610476  |\n",
      "+-------------------------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "#summarizing results\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "\n",
    "table.field_names = [\"Model\", \"LogLoss\"]\n",
    "\n",
    "table.add_row([noevents_model_rand, noevents_rand])\n",
    "table.add_row([noevents_model_lrb, noevents_lrb])\n",
    "table.add_row([noevents_model_svm, noevents_svm])\n",
    "table.add_row([noevents_model_rf, noevents_rf])\n",
    "table.add_row([noevents_model_xgb, noevents_xgb])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_vec = TfidfVectorizer()\n",
    "train_brand = brand_vec.fit_transform(df_events_train['brand'])\n",
    "test_brand = brand_vec.transform(df_events_test['brand'])\n",
    "\n",
    "model_vec = TfidfVectorizer()\n",
    "train_model = model_vec.fit_transform(df_events_train['model'])\n",
    "test_model = model_vec.transform(df_events_test['model'])\n",
    "\n",
    "installed_vec = CountVectorizer()\n",
    "train_installed = installed_vec.fit_transform(df_events_train['installed_apps_string'])\n",
    "test_installed = installed_vec.transform(df_events_test['installed_apps_string'])\n",
    "\n",
    "active_vec = CountVectorizer()\n",
    "train_active = active_vec.fit_transform(df_events_train['active_apps_string'])\n",
    "test_active = active_vec.transform(df_events_test['active_apps_string'])\n",
    "\n",
    "installed_labels_vec = TfidfVectorizer()\n",
    "train_installed_labels = installed_labels_vec.fit_transform(df_events_train['installed_app_labels'])\n",
    "test_installed_labels = installed_labels_vec.transform(df_events_test['installed_app_labels'])\n",
    "\n",
    "active_labels_vec = TfidfVectorizer()\n",
    "train_active_labels = active_labels_vec.fit_transform(df_events_train['active_app_labels'])\n",
    "test_active_labels = active_labels_vec.transform(df_events_test['active_app_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing required inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "age_train = scaler.fit_transform(df_events_train['pred_age'].values.reshape(-1,1))\n",
    "age_test = scaler.transform(df_events_test['pred_age'].values.reshape(-1,1))\n",
    "\n",
    "lat_train = scaler.fit_transform(df_events_train['mean_latitude'].values.reshape(-1,1))\n",
    "lat_test = scaler.transform(df_events_test['mean_latitude'].values.reshape(-1,1))\n",
    "\n",
    "long_train = scaler.fit_transform(df_events_train['mean_longitude'].values.reshape(-1,1))\n",
    "long_test = scaler.transform(df_events_test['mean_longitude'].values.reshape(-1,1))\n",
    "\n",
    "travels_train = scaler.fit_transform(df_events_train['num_travels'].values.reshape(-1,1))\n",
    "travels_test = scaler.transform(df_events_test['num_travels'].values.reshape(-1,1))\n",
    "\n",
    "screen_train = scaler.fit_transform(df_events_train['screen_size'].values.reshape(-1,1))\n",
    "screen_test = scaler.transform(df_events_test['screen_size'].values.reshape(-1,1))\n",
    "\n",
    "ram_train = scaler.fit_transform(df_events_train['ram_gb'].values.reshape(-1,1))\n",
    "ram_test = scaler.transform(df_events_test['ram_gb'].values.reshape(-1,1))\n",
    "\n",
    "camera_train = scaler.fit_transform(df_events_train['camera'].values.reshape(-1,1))\n",
    "camera_test = scaler.transform(df_events_test['camera'].values.reshape(-1,1))\n",
    "\n",
    "release_train = scaler.fit_transform(df_events_train['release_bin'].values.reshape(-1,1))\n",
    "release_test = scaler.transform(df_events_test['release_bin'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = hstack((train_brand, train_model, train_installed_labels, train_active_labels,\n",
    "                  df_events_train['female_pred'].values.reshape(-1,1), df_events_train['male_pred'].values.reshape(-1,1), age_train,\n",
    "                  lat_train, long_train, travels_train, \n",
    "                  np.array(df_events_train['activity_hour'].to_list()), np.array(df_events_train['activity_day'].to_list()), \n",
    "                  df_events_train['app_usage'].values.reshape(-1,1), df_events_train['app_usage_session'].values.reshape(-1,1), \n",
    "                  train_installed, train_active, \n",
    "                  np.array(df_events_train['active_app_usage'].to_list()), screen_train, ram_train,\n",
    "                  camera_train, release_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = hstack((test_brand, test_model, test_installed_labels, test_active_labels,\n",
    "                 df_events_test['female_pred'].values.reshape(-1,1), df_events_test['male_pred'].values.reshape(-1,1), age_test,\n",
    "                 lat_test, long_test, travels_test, \n",
    "                 np.array(df_events_test['activity_hour'].to_list()), np.array(df_events_test['activity_day'].to_list()), \n",
    "                 df_events_test['app_usage'].values.reshape(-1,1), df_events_test['app_usage_session'].values.reshape(-1,1), \n",
    "                 test_installed, test_active, \n",
    "                 np.array(df_events_test['active_app_usage'].to_list()), screen_test, ram_test,\n",
    "                 camera_test, release_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoder = LabelEncoder()\n",
    "\n",
    "y_train = y_encoder.fit_transform(df_events_train['group'])\n",
    "y_test = y_encoder.transform(df_events_test['group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_model_rand = \"Events - Random Model\"\n",
    "\n",
    "predicted_y = np.zeros((len(y_test),12))\n",
    "for i in range(len(y_test)):\n",
    "    rand_probs = np.random.rand(1,12)\n",
    "    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
    "\n",
    "events_rand = log_loss(y_test, predicted_y, labels = list(range(12)), eps=1e-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\sklearn\\model_selection\\_search.py:281: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=8)]: Done  15 out of  27 | elapsed:   34.8s remaining:   27.8s\n",
      "[Parallel(n_jobs=8)]: Done  18 out of  27 | elapsed:   36.1s remaining:   18.0s\n",
      "[Parallel(n_jobs=8)]: Done  21 out of  27 | elapsed:   37.7s remaining:   10.7s\n",
      "[Parallel(n_jobs=8)]: Done  24 out of  27 | elapsed:   38.4s remaining:    4.7s\n",
      "[Parallel(n_jobs=8)]: Done  27 out of  27 | elapsed:   39.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  27 out of  27 | elapsed:   39.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test log loss is: 2.018236536480917\n"
     ]
    }
   ],
   "source": [
    "events_model_lrb = \"Events - Logistic Regression\"\n",
    "\n",
    "hp = {'alpha':[10 ** x for x in range(-6, 3)]}\n",
    "\n",
    "events_lrb = hyperparameter_tuning(\"lrb\", hp, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\sklearn\\model_selection\\_search.py:281: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=8)]: Done  15 out of  27 | elapsed:   34.7s remaining:   27.7s\n",
      "[Parallel(n_jobs=8)]: Done  18 out of  27 | elapsed:   36.2s remaining:   18.1s\n",
      "[Parallel(n_jobs=8)]: Done  21 out of  27 | elapsed:   37.6s remaining:   10.7s\n",
      "[Parallel(n_jobs=8)]: Done  24 out of  27 | elapsed:   38.2s remaining:    4.7s\n",
      "[Parallel(n_jobs=8)]: Done  27 out of  27 | elapsed:   39.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  27 out of  27 | elapsed:   39.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test log loss is: 2.0174097570891036\n"
     ]
    }
   ],
   "source": [
    "events_model_svm = \"Events - Support Vector Machines\"\n",
    "\n",
    "c = {'c':[10 ** x for x in range(-5, 3)]}\n",
    "\n",
    "events_svm = hyperparameter_tuning(\"svm\", hp, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_model_rf = \"Events - Random Forest\"\n",
    "\n",
    "hp = {'n_estimators':[100,200,500,1000,2000],'max_depth':[5, 10]}\n",
    "\n",
    "events_rf = hyperparameter_tuning(\"rf\", hp, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=8)]: Done   9 tasks      | elapsed: 30.1min\n",
      "[Parallel(n_jobs=8)]: Done  19 out of  30 | elapsed: 70.0min remaining: 40.5min\n",
      "[Parallel(n_jobs=8)]: Done  23 out of  30 | elapsed: 81.5min remaining: 24.8min\n",
      "[Parallel(n_jobs=8)]: Done  27 out of  30 | elapsed: 86.6min remaining:  9.6min\n",
      "[Parallel(n_jobs=8)]: Done  30 out of  30 | elapsed: 102.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n...\n",
       "                                           verbosity=None),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=8,\n",
       "                   param_distributions={'colsample_bytree': [0.1, 0.3, 0.5, 1],\n",
       "                                        'learning_rate': [0.01, 0.03, 0.05, 0.1,\n",
       "                                                          0.15, 0.2],\n",
       "                                        'max_depth': [3, 5, 10],\n",
       "                                        'n_estimators': [100, 200, 500, 1000,\n",
       "                                                         2000],\n",
       "                                        'subsample': [0.1, 0.3, 0.5, 1]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='neg_log_loss',\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_model_xgb = \"Events - XGBoost\"\n",
    "\n",
    "x_clf=XGBClassifier()\n",
    "prams={\n",
    "'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n",
    "'n_estimators':[100,200,500,1000,2000],\n",
    "'max_depth':[3,5,10],\n",
    "'colsample_bytree':[0.1,0.3,0.5,1],\n",
    "'subsample':[0.1,0.3,0.5,1]\n",
    "}\n",
    "random_clf=RandomizedSearchCV(x_clf, param_distributions=prams, verbose=10, cv=3, n_jobs=8, scoring = 'neg_log_loss')\n",
    "random_clf.fit(x_train, y_train)\n",
    "\n",
    "x_clf_best = random_clf.best_estimator_\n",
    "predict_y = x_clf_best.predict_proba(x_test)\n",
    "events_xgb = log_loss(y_test, predict_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=12\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(nodes):\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(2048, input_dim=nodes, init='normal', activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(50, input_dim=nodes, init='normal', activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12, init='normal', activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=baseline_model(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2048)              46759936  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                102450    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                612       \n",
      "=================================================================\n",
      "Total params: 46,862,998\n",
      "Trainable params: 46,862,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2048, input_dim=23018, activation=\"tanh\", kernel_initializer=\"normal\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, input_dim=23018, activation=\"tanh\", kernel_initializer=\"normal\")`\n",
      "  import sys\n",
      "C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\Anaconda3\\envs\\appliedai\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 18632 samples, validate on 4658 samples\n",
      "Epoch 1/15\n",
      "18632/18632 [==============================] - 16s 867us/step - loss: 2.4137 - accuracy: 0.1397 - val_loss: 2.3877 - val_accuracy: 0.1501\n",
      "Epoch 2/15\n",
      "18632/18632 [==============================] - 15s 797us/step - loss: 2.3735 - accuracy: 0.1538 - val_loss: 2.3336 - val_accuracy: 0.1621\n",
      "Epoch 3/15\n",
      "18632/18632 [==============================] - 15s 802us/step - loss: 2.3238 - accuracy: 0.1733 - val_loss: 2.2896 - val_accuracy: 0.1913\n",
      "Epoch 4/15\n",
      "18632/18632 [==============================] - 15s 802us/step - loss: 2.2677 - accuracy: 0.1964 - val_loss: 2.2526 - val_accuracy: 0.1986\n",
      "Epoch 5/15\n",
      "18632/18632 [==============================] - 15s 807us/step - loss: 2.2226 - accuracy: 0.2099 - val_loss: 2.1614 - val_accuracy: 0.2325\n",
      "Epoch 6/15\n",
      "18632/18632 [==============================] - 15s 809us/step - loss: 2.1764 - accuracy: 0.2315 - val_loss: 2.1716 - val_accuracy: 0.2344\n",
      "Epoch 7/15\n",
      "18632/18632 [==============================] - 15s 810us/step - loss: 2.1488 - accuracy: 0.2418 - val_loss: 2.1270 - val_accuracy: 0.2447\n",
      "Epoch 8/15\n",
      "18632/18632 [==============================] - 15s 800us/step - loss: 2.1242 - accuracy: 0.2502 - val_loss: 2.0880 - val_accuracy: 0.2754\n",
      "Epoch 9/15\n",
      "18632/18632 [==============================] - 15s 801us/step - loss: 2.1040 - accuracy: 0.2593 - val_loss: 2.0786 - val_accuracy: 0.2587\n",
      "Epoch 10/15\n",
      "18632/18632 [==============================] - 15s 798us/step - loss: 2.0821 - accuracy: 0.2623 - val_loss: 2.1367 - val_accuracy: 0.2572\n",
      "Epoch 11/15\n",
      "18632/18632 [==============================] - 15s 800us/step - loss: 2.0694 - accuracy: 0.2692 - val_loss: 2.1326 - val_accuracy: 0.2508\n",
      "Epoch 12/15\n",
      "18632/18632 [==============================] - 15s 797us/step - loss: 2.0588 - accuracy: 0.2716 - val_loss: 2.0517 - val_accuracy: 0.2677\n",
      "Epoch 13/15\n",
      "18632/18632 [==============================] - 15s 798us/step - loss: 2.0440 - accuracy: 0.2771 - val_loss: 2.1257 - val_accuracy: 0.2548\n",
      "Epoch 14/15\n",
      "18632/18632 [==============================] - 15s 797us/step - loss: 2.0376 - accuracy: 0.2794 - val_loss: 2.1212 - val_accuracy: 0.2623\n",
      "Epoch 15/15\n",
      "18632/18632 [==============================] - 15s 802us/step - loss: 2.0182 - accuracy: 0.2871 - val_loss: 2.0421 - val_accuracy: 0.2838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d73673e240>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model=baseline_model(x_train.shape[1])\n",
    "model.fit(x_train, y_train, epochs=15, batch_size = 64, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test log loss is: 2.0420681420776043\n"
     ]
    }
   ],
   "source": [
    "predict_y = model.predict_proba(x_test)\n",
    "print(\"The test log loss is:\",log_loss(y_test, predict_y))\n",
    "events_nn = log_loss(y_test, predict_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for models with Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------+\n",
      "|              Model               |      LogLoss       |\n",
      "+----------------------------------+--------------------+\n",
      "|      Events - Random Model       | 2.7642369749111952 |\n",
      "|      Events - Random Forest      | 2.018236536480917  |\n",
      "| Events - Support Vector Machines | 2.0174097570891036 |\n",
      "|      Events - Random Forest      | 2.043569156077825  |\n",
      "|         Events - XGBoost         | 1.5679151635707391 |\n",
      "+----------------------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "#summarizing results\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "\n",
    "table.field_names = [\"Model\", \"LogLoss\"]\n",
    "\n",
    "table.add_row([events_model_rand, events_rand])\n",
    "table.add_row([events_model_lrb, events_lrb])\n",
    "table.add_row([events_model_svm, events_svm])\n",
    "table.add_row([events_model_rf, events_rf])\n",
    "table.add_row([events_model_xgb, events_xgb])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------------+--------------------+\n",
      "|          Model          |     No Events      |       Events       |\n",
      "+-------------------------+--------------------+--------------------+\n",
      "|       Random Model      | 2.7587559091491074 | 2.7642369749111952 |\n",
      "|   Logistic Regression   | 2.417876851355263  | 2.018236536480917  |\n",
      "| Support Vector Machines | 2.4178723686862655 | 2.0174097570891036 |\n",
      "|      Random Forest      | 2.3940721530512206 | 2.043569156077825  |\n",
      "|         XGBoost         | 2.396952661610476  | 1.5679151635707391 |\n",
      "|      Neural Network     | 2.428108946999212  | 2.0420681420776043 |\n",
      "+-------------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "table = PrettyTable()\n",
    "\n",
    "table.field_names = [\"Model\", \"No Events\", \"Events\"]\n",
    "\n",
    "table.add_row([\"Random Model\", noevents_rand, events_rand])\n",
    "table.add_row([\"Logistic Regression\", noevents_lrb, events_lrb])\n",
    "table.add_row([\"Support Vector Machines\", noevents_svm, events_svm])\n",
    "table.add_row([\"Random Forest\", noevents_rf, events_rf])\n",
    "table.add_row([\"XGBoost\", noevents_xgb, events_xgb])\n",
    "table.add_row([\"Neural Network\", noevents_nn, events_nn])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "1. All the models outperform a random model.\n",
    "2. There is not much improvement between the models for records with no events.\n",
    "3. But for the records with events, XGBoost drastically outperforms all other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"xgb_model_noevents.pkl\"\n",
    "# save\n",
    "pickle.dump(x_clf_best_noevents, open(file_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"xgb_model_events.pkl\"\n",
    "# save\n",
    "pickle.dump(x_clf_best_events, open(file_name, \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
